{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanviaithal26/Architecting-LLMs-WiDS/blob/main/week3/week3_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#E01: Hyperparameters\n",
        "1.   Block size\n",
        "2.   Embedding Size\n",
        "1.   Number of Neurons in the hidden layer\n",
        "2.   Batch Size\n",
        "1.   Learning Rate Decay\n",
        "2.   Total Iterations\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1SKb7XpD8LKW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "746EpPvt8F9v"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1en9F8G8F9v",
        "outputId": "a0baffad-0daa-4626-cfa2-ed5aafbe15b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-01 05:28:20--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt’\n",
            "\n",
            "\rnames.txt             0%[                    ]       0  --.-KB/s               \rnames.txt           100%[===================>] 222.80K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2026-01-01 05:28:21 (8.38 MB/s) - ‘names.txt’ saved [228145/228145]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# read in all the words\n",
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "words[:8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1yDjo3f8F9w",
        "outputId": "0ebba5ae-75d3-4b27-c407-63dcca5905c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32033"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "len(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JN889LiO8F9w",
        "outputId": "ba22a8f1-3751-4d0b-e14d-39be302f48ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
          ]
        }
      ],
      "source": [
        "# build the vocabulary of characters and mappings to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "print(itos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvCHDuCt8F9x",
        "outputId": "7331e82c-e1fa-4bc2-e77d-f2a9c4ffa323"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ],
      "source": [
        "# build the dataset\n",
        "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
        "\n",
        "def build_dataset(words):\n",
        "  X, Y = [], []\n",
        "  for w in words:\n",
        "\n",
        "    #print(w)\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
        "      context = context[1:] + [ix] # crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr, Ytr = build_dataset(words[:n1])\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])\n",
        "Xte, Yte = build_dataset(words[n2:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "207WWRyZ8F90"
      },
      "outputs": [],
      "source": [
        "# ------------ now made respectable :) ---------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3T2XxYaG8F90",
        "outputId": "61f2a4b6-0270-4591-e48d-b7339fa5af6d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([182625, 3]), torch.Size([182625]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "Xtr.shape, Ytr.shape # dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qTliOIBD8F90"
      },
      "outputs": [],
      "source": [
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "n_hidden = 200                                # 250 hidden neurons\n",
        "C = torch.randn((27, 16), generator=g)        # embedding space of 36\n",
        "W1 = torch.randn((48, n_hidden), generator=g)\n",
        "b1 = torch.randn(n_hidden, generator=g)\n",
        "W2 = torch.randn((n_hidden, 27), generator=g)\n",
        "b2 = torch.randn(27, generator=g)\n",
        "parameters = [C, W1, b1, W2, b2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_PtpQcN8F90",
        "outputId": "62b79161-b937-4b06-fcf9-3a5b8825d27f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15659"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "sum(p.nelement() for p in parameters) # number of parameters in total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "y6Y6ncVd8F91"
      },
      "outputs": [],
      "source": [
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7FncbI6W8F91"
      },
      "outputs": [],
      "source": [
        "lre = torch.linspace(-3, 0, 1000)\n",
        "lrs = 10**lre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8IU_BjgJ8F91"
      },
      "outputs": [],
      "source": [
        "lri = []\n",
        "lossi = []\n",
        "stepi = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "O4Rp0gC58F91"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "iterations = 200000\n",
        "for i in range(iterations):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xtr[ix]] # (128, 4, 36)\n",
        "  h = torch.tanh(emb.view(-1, 48) @ W1 + b1) # (128, 250)\n",
        "  logits = h @ W2 + b2 # (128, 27)\n",
        "  loss = F.cross_entropy(logits, Ytr[ix])\n",
        "  #print(loss.item())\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  #lr = lrs[i]\n",
        "  lr = 0.1 if i < 100000 else 0.01\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  #lri.append(lre[i])\n",
        "  stepi.append(i)\n",
        "  lossi.append(loss.log10().item())\n",
        "\n",
        "#print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fC9Rfa2-8F91",
        "outputId": "ca9ba6e0-91ad-42da-949f-ae4401029b3f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.1358, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "emb = C[Xtr]\n",
        "h = torch.tanh(emb.view(-1, 48) @ W1 + b1)\n",
        "logits = h @ W2 + b2\n",
        "loss = F.cross_entropy(logits, Ytr)\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTo5d-n-8F91",
        "outputId": "c9edd7cb-74d3-47e5-f3bd-728f50436941"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.1784, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "emb = C[Xdev]\n",
        "h = torch.tanh(emb.view(-1, 48) @ W1 + b1)\n",
        "logits = h @ W2 + b2\n",
        "loss = F.cross_entropy(logits, Ydev)\n",
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch size = 4, Embedding = 36, Neurons =250, Batch = 128, Total Iterations = 10000, -----> Overfitted ____________ Training, validation --- 2.02, 2.18"
      ],
      "metadata": {
        "id": "waX80EQP4iKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch size = 4, Embedding = 36, Neurons =250, Batch = 64, Total Iterations = 10000, -----> Even Worse Overfitting"
      ],
      "metadata": {
        "id": "x53MtHk86Nkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch size = 4, Embedding = 16, Neurons =250, Batch = 16, Total Iterations = 10000, -----> Very Little Overfitting, But high loss value --- 2.47/2.49"
      ],
      "metadata": {
        "id": "eTCCP8lw6-AG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch size = 3, Embedding = 16, Neurons =200, Batch = 48, Total Iterations = 20000, -----> Not Much Overfitting, good loss value --- 2.08/2.14"
      ],
      "metadata": {
        "id": "-_wZ0uVC8pEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch size = **3**, Embedding = **16**, Neurons =**200**, Batch = **16**, Total Iterations = **20000**, -----> Not Much Overfitting, Best loss value --- **2.0572/2.1300**"
      ],
      "metadata": {
        "id": "Dl5qy2sT9xx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#E02: Initialisation\n",
        "If the predicted probabilities at initialization were perfectly uniform, then the probability of predicting the next character would be 1/27 for each character. This means that the probability would be -ln(1/27) = 3.2958\n",
        "\n",
        "But the actual predicted loss is around 27, which means the model is confidently wrong. This is because the values randomly assigned to W and b matrices are \"too random\". This loss drops rapidly to near 3 and then the loss gradually reduces. This jump can be avoided by initializing the random values in a normal distribution instead of a truly random pattern."
      ],
      "metadata": {
        "id": "aUHeoyqg_kVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline\n",
        "class Linear:\n",
        "\n",
        "  def __init__(self, fan_in, fan_out, bias=True):\n",
        "    self.weight = torch.zeros((fan_in, fan_out), requires_grad=True)\n",
        "    #self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in**0.5\n",
        "    self.bias = torch.zeros(fan_out, requires_grad=True) if bias else None\n",
        "\n",
        "  def __call__(self, x):\n",
        "    self.out = x @ self.weight\n",
        "    if self.bias is not None:\n",
        "      self.out += self.bias\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
        "\n",
        "\n",
        "class BatchNorm1d:\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.momentum = momentum\n",
        "    self.training = True\n",
        "    # parameters (trained with backprop)\n",
        "    self.gamma = torch.ones(dim, requires_grad=True)\n",
        "    self.beta = torch.zeros(dim, requires_grad=True)\n",
        "    # buffers (trained with a running 'momentum update')\n",
        "    self.running_mean = torch.zeros(dim)\n",
        "    self.running_var = torch.ones(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    if self.training:\n",
        "      xmean = x.mean(0, keepdim=True) # batch mean\n",
        "      xvar = x.var(0, keepdim=True) # batch variance\n",
        "    else:\n",
        "      xmean = self.running_mean\n",
        "      xvar = self.running_var\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    # update the buffers\n",
        "    if self.training:\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
        "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "class Tanh:\n",
        "  def __call__(self, x):\n",
        "    self.out = torch.tanh(x)\n",
        "    return self.out\n",
        "  def parameters(self):\n",
        "    return []\n",
        "\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 100 # the number of neurons in the hidden layer of the MLP\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "vocab_size =27\n",
        "block_size = 3\n",
        "\n",
        "C = torch.randn((vocab_size, n_embd), generator=g, requires_grad=True)\n",
        "\n",
        "layers = [\n",
        "  Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),\n",
        "]\n",
        "# layers = [\n",
        "#   Linear(n_embd * block_size, n_hidden), Tanh(),\n",
        "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
        "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
        "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
        "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
        "#   Linear(           n_hidden, vocab_size),\n",
        "# ]\n",
        "\n",
        "with torch.no_grad():\n",
        "  # last layer: make less confident\n",
        "  layers[-1].gamma *= 0.1\n",
        "  #layers[-1].weight *= 0.1\n",
        "  # all other layers: apply gain\n",
        "  for layer in layers[:-1]:\n",
        "    if isinstance(layer, Linear):\n",
        "      layer.weight *= 1.0 #5/3\n",
        "\n",
        "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LHWTaDsOX2p",
        "outputId": "b796f65b-be30-4a84-92c7-e15565eba5e7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "ud = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xb] # embed the characters into vectors\n",
        "  x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "  for layer in layers:\n",
        "    x = layer(x)\n",
        "  loss = F.cross_entropy(x, Yb) # loss function\n",
        "\n",
        "  for p in parameters:\n",
        "        p.grad = None # Clear old gradients\n",
        "  loss.backward()\n",
        "\n",
        "  lr = 0.1 if i < 100000 else 0.01\n",
        "  for p in parameters:\n",
        "      p.data += -lr * p.grad\n",
        "\n",
        "\n",
        "  # track stats\n",
        "  if i % 10000 == 0: # print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())\n",
        "  with torch.no_grad():\n",
        "    ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])"
      ],
      "metadata": {
        "id": "HyHgKAXW0NiF",
        "outputId": "7cee9a0a-7849-4f65-aabf-8c6f4be8d048",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0/ 200000: 3.2958\n",
            "  10000/ 200000: 2.7882\n",
            "  20000/ 200000: 2.7518\n",
            "  30000/ 200000: 2.8506\n",
            "  40000/ 200000: 2.8221\n",
            "  50000/ 200000: 2.8987\n",
            "  60000/ 200000: 2.6038\n",
            "  70000/ 200000: 2.6751\n",
            "  80000/ 200000: 3.0131\n",
            "  90000/ 200000: 2.7753\n",
            " 100000/ 200000: 2.6079\n",
            " 110000/ 200000: 2.5352\n",
            " 120000/ 200000: 2.6702\n",
            " 130000/ 200000: 2.6442\n",
            " 140000/ 200000: 2.8332\n",
            " 150000/ 200000: 2.5482\n",
            " 160000/ 200000: 2.9762\n",
            " 170000/ 200000: 2.9521\n",
            " 180000/ 200000: 2.8626\n",
            " 190000/ 200000: 2.8505\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Initialising W1 b1 to zeros ^^:\n",
        "\n",
        "If weights and biases are initialised to zero, the network enters a state of symmetry, preventing it from learning correctly. Because every hidden neuron starts at zero, receives the same input, and produces identical tanh(0)=0 activations, they all receive identical gradients. The entire hidden layer (e.g., 200 neurons) behaves like a single neuron. However, the Embedding Layer (C) and Output Layer (W2, b2) can still differentiate because they have their own random initializations.\n",
        "\n",
        "It is seen above that the loss moves randomly and does not decrease as it should be.\n"
      ],
      "metadata": {
        "id": "79bP45op0Obg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BatchNorm 3-layer MLP\n"
      ],
      "metadata": {
        "id": "5QkisaHo2iLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Linear:\n",
        "  def __init__(self, fan_in, fan_out, bias=True):\n",
        "    # Kaiming Initialization: scale by 1/sqrt(fan_in)\n",
        "    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5\n",
        "    self.bias = torch.zeros(fan_out) if bias else None\n",
        "\n",
        "  def __call__(self, x):\n",
        "    self.out = x @ self.weight\n",
        "    if self.bias is not None:\n",
        "      self.out += self.bias\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
        "\n",
        "class BatchNorm1d:\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.momentum = momentum\n",
        "    self.training = True\n",
        "    self.gamma = torch.ones(dim) # Scale parameter\n",
        "    self.beta = torch.zeros(dim)  # Shift parameter\n",
        "    self.running_mean = torch.zeros(dim) # Buffer\n",
        "    self.running_var = torch.ones(dim)   # Buffer\n",
        "\n",
        "  def __call__(self, x):\n",
        "    if self.training:\n",
        "      xmean = x.mean(0, keepdim=True)\n",
        "      xvar = x.var(0, keepdim=True)\n",
        "    else:\n",
        "      xmean = self.running_mean\n",
        "      xvar = self.running_var\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # Normalize\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    if self.training:\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
        "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "class Tanh:\n",
        "  def __call__(self, x):\n",
        "    self.out = torch.tanh(x)\n",
        "    return self.out\n",
        "  def parameters(self):\n",
        "    return []"
      ],
      "metadata": {
        "id": "iRS0PF5B2uSS"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set to eval mode to use running buffers\n",
        "for layer in layers:\n",
        "    if isinstance(layer, BatchNorm1d):\n",
        "        layer.training = False\n",
        "\n",
        "# 1. Extract original parameters\n",
        "W = layers[0].weight\n",
        "b = torch.zeros(W.shape[1]) # b was None because bias=False\n",
        "\n",
        "# 2. Extract BatchNorm parameters\n",
        "gamma = layers[1].gamma\n",
        "beta = layers[1].beta\n",
        "mu = layers[1].running_mean\n",
        "var = layers[1].running_var\n",
        "eps = layers[1].eps\n",
        "\n",
        "# 3. Calculate folded weight and bias\n",
        "scale = gamma / torch.sqrt(var + eps)\n",
        "W_folded = W * scale\n",
        "b_folded = beta + (b - mu) * scale\n",
        "\n",
        "# 4. Verify the forward pass\n",
        "# Original forward path\n",
        "x_test = emb.view(emb.shape[0], -1)\n",
        "out_original = layers[1](layers[0](x_test))\n",
        "\n",
        "# Fused forward path\n",
        "out_fused = x_test @ W_folded + b_folded\n",
        "\n",
        "# Check closeness\n",
        "max_diff = (out_original - out_fused).abs().max().item()\n",
        "print(f\"Folding successful! Max difference: {max_diff:.4e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXstkhss2zzY",
        "outputId": "3a2d9434-0682-4e78-ff09-4a828b146031"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folding successful! Max difference: 0.0000e+00\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}