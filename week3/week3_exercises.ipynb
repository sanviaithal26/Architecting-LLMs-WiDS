{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanviaithal26/Architecting-LLMs-WiDS/blob/main/week3/week3_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#E01: Hyperparameters\n",
        "1.   Block size\n",
        "2.   Embedding Size\n",
        "1.   Number of Neurons in the hidden layer\n",
        "2.   Batch Size\n",
        "1.   Learning Rate Decay\n",
        "2.   Total Iterations\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1SKb7XpD8LKW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "746EpPvt8F9v"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1en9F8G8F9v",
        "outputId": "8d8a78da-3a48-4b61-d935-fa66ea718d0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-03 17:12:03--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt.1’\n",
            "\n",
            "\rnames.txt.1           0%[                    ]       0  --.-KB/s               \rnames.txt.1         100%[===================>] 222.80K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2026-01-03 17:12:03 (10.5 MB/s) - ‘names.txt.1’ saved [228145/228145]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# read in all the words\n",
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "words[:8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1yDjo3f8F9w",
        "outputId": "fca08c1e-4554-4c34-c5da-6f3857df285e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32033"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "len(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JN889LiO8F9w",
        "outputId": "220e5325-2d01-4e05-bfac-48e9cb2f6886"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
          ]
        }
      ],
      "source": [
        "# build the vocabulary of characters and mappings to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "print(itos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvCHDuCt8F9x",
        "outputId": "2d15169a-f07b-4eaa-95cb-9f3cee0aa7da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ],
      "source": [
        "# build the dataset\n",
        "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
        "\n",
        "def build_dataset(words):\n",
        "  X, Y = [], []\n",
        "  for w in words:\n",
        "\n",
        "    #print(w)\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
        "      context = context[1:] + [ix] # crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr, Ytr = build_dataset(words[:n1])\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])\n",
        "Xte, Yte = build_dataset(words[n2:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "207WWRyZ8F90"
      },
      "outputs": [],
      "source": [
        "# ------------ now made respectable :) ---------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3T2XxYaG8F90"
      },
      "outputs": [],
      "source": [
        "Xtr.shape, Ytr.shape # dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qTliOIBD8F90"
      },
      "outputs": [],
      "source": [
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "n_hidden = 200                                # 250 hidden neurons\n",
        "C = torch.randn((27, 16), generator=g)        # embedding space of 36\n",
        "W1 = torch.randn((48, n_hidden), generator=g)\n",
        "b1 = torch.randn(n_hidden, generator=g)\n",
        "W2 = torch.randn((n_hidden, 27), generator=g)\n",
        "b2 = torch.randn(27, generator=g)\n",
        "parameters = [C, W1, b1, W2, b2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_PtpQcN8F90",
        "outputId": "3543605a-307e-4377-9d9f-7487e6f99734"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15659"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "sum(p.nelement() for p in parameters) # number of parameters in total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "y6Y6ncVd8F91"
      },
      "outputs": [],
      "source": [
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "7FncbI6W8F91"
      },
      "outputs": [],
      "source": [
        "lre = torch.linspace(-3, 0, 1000)\n",
        "lrs = 10**lre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "8IU_BjgJ8F91"
      },
      "outputs": [],
      "source": [
        "lri = []\n",
        "lossi = []\n",
        "stepi = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "O4Rp0gC58F91",
        "outputId": "98add948-be45-4743-eab4-d53aac5e78a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (10x48 and 30x200)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3187397975.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mXtr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# (128, 4, 36)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m48\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mW1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (128, 250)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mW2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m \u001b[0;31m# (128, 27)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10x48 and 30x200)"
          ]
        }
      ],
      "source": [
        "batch_size = 16\n",
        "iterations = 200000\n",
        "for i in range(iterations):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xtr[ix]] # (128, 4, 36)\n",
        "  h = torch.tanh(emb.view(-1, 48) @ W1 + b1) # (128, 250)\n",
        "  logits = h @ W2 + b2 # (128, 27)\n",
        "  loss = F.cross_entropy(logits, Ytr[ix])\n",
        "  #print(loss.item())\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  #lr = lrs[i]\n",
        "  lr = 0.1 if i < 100000 else 0.01\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  #lri.append(lre[i])\n",
        "  stepi.append(i)\n",
        "  lossi.append(loss.log10().item())\n",
        "\n",
        "#print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fC9Rfa2-8F91",
        "outputId": "da8e91ce-0369-47a3-dddc-24068e58a3fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.1372, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "emb = C[Xtr]\n",
        "h = torch.tanh(emb.view(-1, 48) @ W1 + b1)\n",
        "logits = h @ W2 + b2\n",
        "loss = F.cross_entropy(logits, Ytr)\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTo5d-n-8F91",
        "outputId": "a24c005e-8654-42fc-f0ec-e8a89ef8449a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.1871, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "emb = C[Xdev]\n",
        "h = torch.tanh(emb.view(-1, 48) @ W1 + b1)\n",
        "logits = h @ W2 + b2\n",
        "loss = F.cross_entropy(logits, Ydev)\n",
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch size = 4, Embedding = 36, Neurons =250, Batch = 128, Total Iterations = 10000, -----> Overfitted ____________ Training, validation --- 2.02, 2.18"
      ],
      "metadata": {
        "id": "waX80EQP4iKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch size = 4, Embedding = 36, Neurons =250, Batch = 64, Total Iterations = 10000, -----> Even Worse Overfitting"
      ],
      "metadata": {
        "id": "x53MtHk86Nkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch size = 4, Embedding = 16, Neurons =250, Batch = 16, Total Iterations = 10000, -----> Very Little Overfitting, But high loss value --- 2.47/2.49"
      ],
      "metadata": {
        "id": "eTCCP8lw6-AG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch size = 3, Embedding = 16, Neurons =200, Batch = 48, Total Iterations = 20000, -----> Not Much Overfitting, good loss value --- 2.08/2.14"
      ],
      "metadata": {
        "id": "-_wZ0uVC8pEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch size = **3**, Embedding = **16**, Neurons =**200**, Batch = **16**, Total Iterations = **20000**, -----> Not Much Overfitting, Best loss value --- **2.0572/2.1300**"
      ],
      "metadata": {
        "id": "Dl5qy2sT9xx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Finally, run from the beginning ---> 2.14/2.19"
      ],
      "metadata": {
        "id": "sqhLAb5w3Cfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#E02: Initialisation\n",
        "If the predicted probabilities at initialization were perfectly uniform, then the probability of predicting the next character would be 1/27 for each character. This means that the probability would be -ln(1/27) = 3.2958\n",
        "\n",
        "But the actual predicted loss is around 27, which means the model is confidently wrong. This is because the values randomly assigned to W and b matrices are \"too random\". This loss drops rapidly to near 3 and then the loss gradually reduces. This jump can be avoided by initializing the random values in a normal distribution instead of a truly random pattern."
      ],
      "metadata": {
        "id": "aUHeoyqg_kVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#E03 : Bengio et al\n",
        "In the Bengio et al. 2003 paper, one of the key architectural ideas proposed  is the use of direct connections from the input to the output layer. In the standard makemore MLP, the architecture is: Input $\\rightarrow$ Embeddings $\\rightarrow$ Hidden Layer (tanh) $\\rightarrow$ Output (logits).\n",
        " Bengio's \"Direct Connection\" adds a path that bypasses the hidden layer:2.   Input $\\rightarrow$ Embeddings $\\rightarrow$ Output (logits).The final output is the sum of these two paths: $y = b + Wx + U \\tanh(d + Hx)$, where $Wx$ represents the direct connection."
      ],
      "metadata": {
        "id": "bsa3F7zE0oQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Parameters setup\n",
        "n_embd = 10\n",
        "n_hidden = 200\n",
        "block_size = 3 # context length\n",
        "vocab_size = 27\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "C = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) # Kaiming init\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.01\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0\n",
        "\n",
        "# --- Bengio 2003 Direct Connection ---\n",
        "W_direct = torch.randn((n_embd * block_size, vocab_size), generator=g) * 0.01\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, W_direct]\n",
        "for p in parameters:\n",
        "    p.requires_grad = True"
      ],
      "metadata": {
        "id": "9T2Kl01O1bSn"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(200000):\n",
        "\n",
        "    # Minibatch construct\n",
        "    ix = torch.randint(0, Xtr.shape[0], (32,), generator=g)\n",
        "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X, Y\n",
        "\n",
        "    # Forward pass\n",
        "    emb = C[Xb] # (batch_size, block_size, n_embd)\n",
        "    x = emb.view(emb.shape[0], -1) # flatten to (batch_size, block_size * n_embd)\n",
        "\n",
        "    # Path A: The Hidden Layer (MLP)\n",
        "    hpreact = x @ W1 + b1\n",
        "    h = torch.tanh(hpreact)\n",
        "    logits_hidden = h @ W2 + b2\n",
        "\n",
        "    # Path B: Bengio's Direct Connection (Linear)\n",
        "    logits_direct = x @ W_direct\n",
        "\n",
        "    # Combined Logits\n",
        "    logits = logits_hidden + logits_direct\n",
        "\n",
        "    # Loss\n",
        "    loss = F.cross_entropy(logits, Yb)\n",
        "\n",
        "    # Backward pass\n",
        "    for p in parameters:\n",
        "        p.grad = None\n",
        "    loss.backward()\n",
        "\n",
        "    # Update\n",
        "    lr = 0.1 if i < 100000 else 0.01\n",
        "    for p in parameters:\n",
        "        p.data += -lr * p.grad\n",
        "\n",
        "    if i % 10000 == 0:\n",
        "        print(f'{i:7d}/{200000:7d}: {loss.item():.4f}')"
      ],
      "metadata": {
        "id": "j1hP28co1Z3g",
        "outputId": "63bc464f-07bb-4cc4-db18-65882879b56b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0/ 200000: 3.3333\n",
            "  10000/ 200000: 2.1722\n",
            "  20000/ 200000: 2.3112\n",
            "  30000/ 200000: 2.3336\n",
            "  40000/ 200000: 2.2141\n",
            "  50000/ 200000: 2.7633\n",
            "  60000/ 200000: 2.1674\n",
            "  70000/ 200000: 2.1255\n",
            "  80000/ 200000: 2.2769\n",
            "  90000/ 200000: 2.3364\n",
            " 100000/ 200000: 2.1920\n",
            " 110000/ 200000: 2.0001\n",
            " 120000/ 200000: 2.3169\n",
            " 130000/ 200000: 1.7901\n",
            " 140000/ 200000: 1.8236\n",
            " 150000/ 200000: 2.0944\n",
            " 160000/ 200000: 2.3345\n",
            " 170000/ 200000: 1.9606\n",
            " 180000/ 200000: 2.0588\n",
            " 190000/ 200000: 1.7269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def split_loss(split):\n",
        "    x, y = {\n",
        "        'train': (Xtr, Ytr),\n",
        "        'val': (Xdev, Ydev),\n",
        "        'test': (Xte, Yte),\n",
        "    }[split]\n",
        "\n",
        "    emb = C[x]\n",
        "    flattened_x = emb.view(emb.shape[0], -1)\n",
        "\n",
        "    # Forward pass with direct connection\n",
        "    h = torch.tanh(flattened_x @ W1 + b1)\n",
        "    logits = (h @ W2 + b2) + (flattened_x @ W_direct) # Combined paths\n",
        "\n",
        "    loss = F.cross_entropy(logits, y)\n",
        "    print(f'{split} loss: {loss.item():.4f}')\n",
        "\n",
        "split_loss('val')"
      ],
      "metadata": {
        "id": "4kpUkDLU1RSV",
        "outputId": "46073636-ae86-441c-ce56-ed8deb6a7bd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 2.1056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The model trained faster, and produced quite a lower loss of 2.11, which shows that the concept of direct connections worked."
      ],
      "metadata": {
        "id": "PKFN4-Fp5JmU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 2\n"
      ],
      "metadata": {
        "id": "E59Jf8-W1A5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##E01: Initialising W1 b1 to zeros:\n",
        "\n",
        "If weights and biases are initialised to zero, the network enters a state of symmetry, preventing it from learning correctly. Because every hidden neuron starts at zero, receives the same input, and produces identical tanh(0)=0 activations, they all receive identical gradients. The entire hidden layer (e.g., 200 neurons) behaves like a single neuron. However, the Embedding Layer (C) and Output Layer (W2, b2) can still differentiate because they have their own random initializations.\n",
        "\n",
        "It is seen above that the loss moves randomly and does not decrease as it should be.\n"
      ],
      "metadata": {
        "id": "79bP45op0Obg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline\n",
        "class Linear:\n",
        "\n",
        "  def __init__(self, fan_in, fan_out, bias=True):\n",
        "    self.weight = torch.zeros((fan_in, fan_out), requires_grad=True)\n",
        "    #self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in**0.5\n",
        "    self.bias = torch.zeros(fan_out, requires_grad=True) if bias else None\n",
        "\n",
        "  def __call__(self, x):\n",
        "    self.out = x @ self.weight\n",
        "    if self.bias is not None:\n",
        "      self.out += self.bias\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
        "\n",
        "\n",
        "class BatchNorm1d:\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.momentum = momentum\n",
        "    self.training = True\n",
        "    # parameters (trained with backprop)\n",
        "    self.gamma = torch.ones(dim, requires_grad=True)\n",
        "    self.beta = torch.zeros(dim, requires_grad=True)\n",
        "    # buffers (trained with a running 'momentum update')\n",
        "    self.running_mean = torch.zeros(dim)\n",
        "    self.running_var = torch.ones(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    if self.training:\n",
        "      xmean = x.mean(0, keepdim=True) # batch mean\n",
        "      xvar = x.var(0, keepdim=True) # batch variance\n",
        "    else:\n",
        "      xmean = self.running_mean\n",
        "      xvar = self.running_var\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    # update the buffers\n",
        "    if self.training:\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
        "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "class Tanh:\n",
        "  def __call__(self, x):\n",
        "    self.out = torch.tanh(x)\n",
        "    return self.out\n",
        "  def parameters(self):\n",
        "    return []\n",
        "\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 100 # the number of neurons in the hidden layer of the MLP\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "vocab_size =27\n",
        "block_size = 3\n",
        "\n",
        "C = torch.randn((vocab_size, n_embd), generator=g, requires_grad=True)\n",
        "\n",
        "layers = [\n",
        "  Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),\n",
        "]\n",
        "# layers = [\n",
        "#   Linear(n_embd * block_size, n_hidden), Tanh(),\n",
        "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
        "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
        "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
        "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
        "#   Linear(           n_hidden, vocab_size),\n",
        "# ]\n",
        "\n",
        "with torch.no_grad():\n",
        "  # last layer: make less confident\n",
        "  layers[-1].gamma *= 0.1\n",
        "  #layers[-1].weight *= 0.1\n",
        "  # all other layers: apply gain\n",
        "  for layer in layers[:-1]:\n",
        "    if isinstance(layer, Linear):\n",
        "      layer.weight *= 1.0 #5/3\n",
        "\n",
        "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LHWTaDsOX2p",
        "outputId": "84dc3d39-c22c-4488-bd4d-9641af557355"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "ud = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xb] # embed the characters into vectors\n",
        "  x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "  for layer in layers:\n",
        "    x = layer(x)\n",
        "  loss = F.cross_entropy(x, Yb) # loss function\n",
        "\n",
        "  for p in parameters:\n",
        "        p.grad = None # Clear old gradients\n",
        "  loss.backward()\n",
        "\n",
        "  lr = 0.1 if i < 100000 else 0.01\n",
        "  for p in parameters:\n",
        "      p.data += -lr * p.grad\n",
        "\n",
        "\n",
        "  # track stats\n",
        "  if i % 10000 == 0: # print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())\n",
        "  with torch.no_grad():\n",
        "    ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])"
      ],
      "metadata": {
        "id": "HyHgKAXW0NiF",
        "outputId": "0b8ccdc1-675d-45b3-978f-df8d8e1f3f2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0/ 200000: 3.2958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##E02: BatchNorm 3-layer MLP\n"
      ],
      "metadata": {
        "id": "5QkisaHo2iLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Linear:\n",
        "  def __init__(self, fan_in, fan_out, bias=True):\n",
        "    # Kaiming Initialization: scale by 1/sqrt(fan_in)\n",
        "    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5\n",
        "    self.bias = torch.zeros(fan_out) if bias else None\n",
        "\n",
        "  def __call__(self, x):\n",
        "    self.out = x @ self.weight\n",
        "    if self.bias is not None:\n",
        "      self.out += self.bias\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
        "\n",
        "class BatchNorm1d:\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.momentum = momentum\n",
        "    self.training = True\n",
        "    self.gamma = torch.ones(dim) # Scale parameter\n",
        "    self.beta = torch.zeros(dim)  # Shift parameter\n",
        "    self.running_mean = torch.zeros(dim) # Buffer\n",
        "    self.running_var = torch.ones(dim)   # Buffer\n",
        "\n",
        "  def __call__(self, x):\n",
        "    if self.training:\n",
        "      xmean = x.mean(0, keepdim=True)\n",
        "      xvar = x.var(0, keepdim=True)\n",
        "    else:\n",
        "      xmean = self.running_mean\n",
        "      xvar = self.running_var\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # Normalize\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    if self.training:\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
        "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "class Tanh:\n",
        "  def __call__(self, x):\n",
        "    self.out = torch.tanh(x)\n",
        "    return self.out\n",
        "  def parameters(self):\n",
        "    return []"
      ],
      "metadata": {
        "id": "iRS0PF5B2uSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set to eval mode to use running buffers\n",
        "for layer in layers:\n",
        "    if isinstance(layer, BatchNorm1d):\n",
        "        layer.training = False\n",
        "\n",
        "# 1. Extract original parameters\n",
        "W = layers[0].weight\n",
        "b = torch.zeros(W.shape[1]) # b was None because bias=False\n",
        "\n",
        "# 2. Extract BatchNorm parameters\n",
        "gamma = layers[1].gamma\n",
        "beta = layers[1].beta\n",
        "mu = layers[1].running_mean\n",
        "var = layers[1].running_var\n",
        "eps = layers[1].eps\n",
        "\n",
        "# 3. Calculate folded weight and bias\n",
        "scale = gamma / torch.sqrt(var + eps)\n",
        "W_folded = W * scale\n",
        "b_folded = beta + (b - mu) * scale\n",
        "\n",
        "# 4. Verify the forward pass\n",
        "# Original forward path\n",
        "x_test = emb.view(emb.shape[0], -1)\n",
        "out_original = layers[1](layers[0](x_test))\n",
        "\n",
        "# Fused forward path\n",
        "out_fused = x_test @ W_folded + b_folded\n",
        "\n",
        "# Check closeness\n",
        "max_diff = (out_original - out_fused).abs().max().item()\n",
        "print(f\"Folding successful! Max difference: {max_diff:.4e}\")"
      ],
      "metadata": {
        "id": "iXstkhss2zzY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}