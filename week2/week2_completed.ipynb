{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhKzvY3Fh+8FF3jmFE5JJH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanviaithal26/Architecting-LLMs-WiDS/blob/main/week2_completed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E01 : Trigram"
      ],
      "metadata": {
        "id": "p88zf3LWAz58"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rG6erXHgh9DF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbe65b31-ecc5-4228-a945-1e9e7c7c9b44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-21 17:40:57--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt.11’\n",
            "\n",
            "names.txt.11        100%[===================>] 222.80K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2025-12-21 17:40:57 (74.3 MB/s) - ‘names.txt.11’ saved [228145/228145]\n",
            "\n",
            "Iteration 0: loss = 3.7375\n",
            "Iteration 10: loss = 3.1778\n",
            "Iteration 20: loss = 2.9177\n",
            "Iteration 30: loss = 2.7716\n",
            "Iteration 40: loss = 2.6733\n",
            "Iteration 50: loss = 2.6008\n",
            "Iteration 60: loss = 2.5447\n",
            "Iteration 70: loss = 2.4999\n",
            "Iteration 80: loss = 2.4633\n",
            "Iteration 90: loss = 2.4330\n",
            "den\n",
            "gislwqxlkonni\n",
            "dari\n",
            "chrqagrvzhbibocqilkharj\n",
            "ixamarkudxhce\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
        "\n",
        "# 1. Load the data\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "\n",
        "# 2. Build the vocabulary and mappings\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "\n",
        "# 3. Create the dataset (xs: input character, ys: target next character)\n",
        "xs, ys = [], []\n",
        "for w in words:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "        ix1, ix2, ix3 = stoi[ch1], stoi[ch2], stoi[ch3]\n",
        "        xs.append(27*ix1 + ix2)\n",
        "        ys.append(ix3)\n",
        "\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)\n",
        "num = len(xs)\n",
        "\n",
        "# 4. Initialize the neural network (one-layer, 27 neurons)\n",
        "# We use a 729x27 weight matrix (mapping 729 inputs to 27 outputs)\n",
        "g = torch.Generator().manual_seed(2147483647+1)\n",
        "W = torch.randn((729, 27), generator=g, requires_grad=True)\n",
        "\n",
        "# 5. Gradient Descent (Training Loop)\n",
        "for k in range(100):\n",
        "\n",
        "    # Forward pass\n",
        "    xenc = F.one_hot(xs, num_classes=729).float() # input to the network: one-hot encoding\n",
        "    logits = xenc @ W # predict log-counts\n",
        "    counts = logits.exp() # counts, equivalent to N matrix\n",
        "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "\n",
        "    # Loss: negative log-likelihood (NLL)\n",
        "    # We select the probability assigned to the actual next character\n",
        "    loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean() # Includes small regularizer\n",
        "\n",
        "    # Backward pass\n",
        "    W.grad = None # set gradient to zero\n",
        "    loss.backward()\n",
        "\n",
        "    # Update weights\n",
        "    W.data += -50 * W.grad\n",
        "\n",
        "    if k % 10 == 0:\n",
        "        print(f\"Iteration {k}: loss = {loss.item():.4f}\")\n",
        "\n",
        "# 6. Sampling from the model\n",
        "for i in range(5):\n",
        "    out = []\n",
        "    prev_two = [0,0]\n",
        "    while True:\n",
        "        xenc = F.one_hot(torch.tensor([27*prev_two[0] + prev_two[1]]), num_classes=729).float()\n",
        "        logits = xenc @ W\n",
        "        counts = logits.exp()\n",
        "        p = counts / counts.sum(1, keepdims=True)\n",
        "\n",
        "        next_ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
        "        if next_ix == 0:\n",
        "            break\n",
        "        out.append(itos[next_ix])\n",
        "        prev_two = [prev_two[1], next_ix]\n",
        "    print(''.join(out))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E02.1: Testing Bigram on Test and Dev Sets"
      ],
      "metadata": {
        "id": "PadKL2v1BQYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "# 1. Load the data\n",
        "words_unshuffled= open('names.txt', 'r').read().splitlines()\n",
        "num_words = len(words_unshuffled)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*num_words)\n",
        "n2 = int(0.9*num_words)\n",
        "train_set = words[:n1]\n",
        "dev_set = words[n1:n2]\n",
        "test_set = words[n2:]"
      ],
      "metadata": {
        "id": "o2a2P9cuA9Eo"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2. Build the vocabulary and mappings\n",
        "chars = sorted(list(set(''.join(train_set))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "\n",
        "# 3. Create the dataset (xs: input character, ys: target next character)\n",
        "xs, ys = [], []\n",
        "for w in train_set:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for ch1, ch2 in zip(chs, chs[1:]):\n",
        "        ix1, ix2 = stoi[ch1], stoi[ch2]\n",
        "        xs.append(ix1)\n",
        "        ys.append(ix2)\n",
        "\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)\n",
        "num = xs.nelement()\n",
        "\n",
        "# 4. Initialize the neural network (one-layer, 27 neurons)\n",
        "# We use a 27x27 weight matrix (mapping 27 inputs to 27 outputs)\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((27, 27), generator=g, requires_grad=True)\n",
        "\n",
        "# 5. Gradient Descent (Training Loop)\n",
        "for k in range(100):\n",
        "\n",
        "    # Forward pass\n",
        "    xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
        "    logits = xenc @ W # predict log-counts\n",
        "    counts = logits.exp() # counts, equivalent to N matrix\n",
        "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "\n",
        "    # Loss: negative log-likelihood (NLL)\n",
        "    # We select the probability assigned to the actual next character\n",
        "    loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean() # Includes small regularizer\n",
        "\n",
        "    # Backward pass\n",
        "    W.grad = None # set gradient to zero\n",
        "    loss.backward()\n",
        "\n",
        "    # Update weights\n",
        "    W.data += -50 * W.grad\n",
        "\n",
        "    if k % 10 == 0:\n",
        "        print(f\"Iteration {k}: loss = {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGfsY968Ju9e",
        "outputId": "b176b7ed-d193-4f04-bbcc-106330c34e6b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: loss = 3.7681\n",
            "Iteration 10: loss = 2.6966\n",
            "Iteration 20: loss = 2.5828\n",
            "Iteration 30: loss = 2.5421\n",
            "Iteration 40: loss = 2.5220\n",
            "Iteration 50: loss = 2.5105\n",
            "Iteration 60: loss = 2.5033\n",
            "Iteration 70: loss = 2.4984\n",
            "Iteration 80: loss = 2.4949\n",
            "Iteration 90: loss = 2.4924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xsd, ysd = [], []\n",
        "for w in dev_set:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for ch1, ch2 in zip(chs, chs[1:]):\n",
        "        ix1, ix2 = stoi[ch1], stoi[ch2]\n",
        "        xsd.append(ix1)\n",
        "        ysd.append(ix2)\n",
        "\n",
        "xsd = torch.tensor(xsd)\n",
        "ysd = torch.tensor(ysd)\n",
        "num = xsd.nelement()\n",
        "\n",
        " # Forward pass\n",
        "xenc = F.one_hot(xsd, num_classes=27).float() # input to the network: one-hot encoding\n",
        "logits = xenc @ W # predict log-counts\n",
        "counts = logits.exp() # counts, equivalent to N matrix\n",
        "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "\n",
        "    # Loss: negative log-likelihood (NLL)\n",
        "    # We select the probability assigned to the actual next character\n",
        "lossd = -probs[torch.arange(num), ysd].log().mean() + 0.01*(W**2).mean() # Includes small regularizer\n",
        "print(f\"Dev set loss: {lossd.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OcQWnA3KKzz",
        "outputId": "7ab04f05-e493-4f68-ed6a-ac904977169d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev set loss: 2.4840035438537598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xst, yst = [], []\n",
        "for w in test_set:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for ch1, ch2 in zip(chs, chs[1:]):\n",
        "        ix1, ix2 = stoi[ch1], stoi[ch2]\n",
        "        xst.append(ix1)\n",
        "        yst.append(ix2)\n",
        "\n",
        "xst = torch.tensor(xst)\n",
        "yst = torch.tensor(yst)\n",
        "num = xst.nelement()\n",
        "\n",
        " # Forward pass\n",
        "xenc = F.one_hot(xst, num_classes=27).float() # input to the network: one-hot encoding\n",
        "logits = xenc @ W # predict log-counts\n",
        "counts = logits.exp() # counts, equivalent to N matrix\n",
        "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "\n",
        "    # Loss: negative log-likelihood (NLL)\n",
        "    # We select the probability assigned to the actual next character\n",
        "losst = -probs[torch.arange(num), yst].log().mean() + 0.01*(W**2).mean() # Includes small regularizer\n",
        "print(f\"Test set loss: {losst.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q99tumHnMcZ8",
        "outputId": "1c7636ea-9b4f-41b0-cf5b-fd170b737add"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev set loss: 2.492913246154785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final loss with test and dev sets is similar to that of training set. This shows there's no particular overfitting that's observed."
      ],
      "metadata": {
        "id": "VEnK-x2hRCpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E02.2: Testing Trigram on Test and Dev Sets"
      ],
      "metadata": {
        "id": "Noh8VRXCRTs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "# 1. Load the data\n",
        "words_unshuffled= open('names.txt', 'r').read().splitlines()\n",
        "num_words = len(words_unshuffled)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*num_words)\n",
        "n2 = int(0.9*num_words)\n",
        "train_set = words[:n1]\n",
        "dev_set = words[n1:n2]\n",
        "test_set = words[n2:]"
      ],
      "metadata": {
        "id": "YENoADIyRoAc"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Build the vocabulary and mappings\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "\n",
        "# 3. Create the dataset (xs: input character, ys: target next character)\n",
        "xs, ys = [], []\n",
        "for w in train_set:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "        ix1, ix2, ix3 = stoi[ch1], stoi[ch2], stoi[ch3]\n",
        "        xs.append(27*ix1 + ix2)\n",
        "        ys.append(ix3)\n",
        "\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)\n",
        "num = len(xs)\n",
        "\n",
        "# 4. Initialize the neural network (one-layer, 27 neurons)\n",
        "# We use a 729x27 weight matrix (mapping 729 inputs to 27 outputs)\n",
        "g = torch.Generator().manual_seed(2147483647+1)\n",
        "W = torch.randn((729, 27), generator=g, requires_grad=True)\n",
        "\n",
        "# 5. Gradient Descent (Training Loop)\n",
        "for k in range(100):\n",
        "\n",
        "    # Forward pass\n",
        "    xenc = F.one_hot(xs, num_classes=729).float() # input to the network: one-hot encoding\n",
        "    logits = xenc @ W # predict log-counts\n",
        "    counts = logits.exp() # counts, equivalent to N matrix\n",
        "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "\n",
        "    # Loss: negative log-likelihood (NLL)\n",
        "    # We select the probability assigned to the actual next character\n",
        "    loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean() # Includes small regularizer\n",
        "\n",
        "    # Backward pass\n",
        "    W.grad = None # set gradient to zero\n",
        "    loss.backward()\n",
        "\n",
        "    # Update weights\n",
        "    W.data += -50 * W.grad\n",
        "\n",
        "    if k % 10 == 0:\n",
        "        print(f\"Iteration {k}: loss = {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s65DebSBRgOF",
        "outputId": "ae0e2549-ea53-4407-b1b2-993c3ff25374"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: loss = 3.7382\n",
            "Iteration 10: loss = 3.1798\n",
            "Iteration 20: loss = 2.9198\n",
            "Iteration 30: loss = 2.7734\n",
            "Iteration 40: loss = 2.6748\n",
            "Iteration 50: loss = 2.6020\n",
            "Iteration 60: loss = 2.5457\n",
            "Iteration 70: loss = 2.5006\n",
            "Iteration 80: loss = 2.4638\n",
            "Iteration 90: loss = 2.4333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xsd, ysd = [], []\n",
        "for w in dev_set:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "        ix1, ix2, ix3 = stoi[ch1], stoi[ch2], stoi[ch3]\n",
        "        xsd.append(27*ix1 + ix2)\n",
        "        ysd.append(ix3)\n",
        "\n",
        "xsd = torch.tensor(xsd)\n",
        "ysd = torch.tensor(ysd)\n",
        "num = len(xsd)\n",
        "\n",
        "# Forward pass\n",
        "xenc = F.one_hot(xsd, num_classes=729).float() # input to the network: one-hot encoding\n",
        "logits = xenc @ W # predict log-counts\n",
        "counts = logits.exp() # counts, equivalent to N matrix\n",
        "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "\n",
        "    # Loss: negative log-likelihood (NLL)\n",
        "    # We select the probability assigned to the actual next character\n",
        "lossd = -probs[torch.arange(num), ysd].log().mean() + 0.01*(W**2).mean() # Includes small regularizer\n",
        "print(f\"Dev set loss: {lossd.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDpofOg9SHWr",
        "outputId": "183e3fd8-7bbe-428a-fb5a-50a13fce95e4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev set loss: 2.4210987091064453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xst, yst = [], []\n",
        "for w in test_set:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "        ix1, ix2, ix3 = stoi[ch1], stoi[ch2], stoi[ch3]\n",
        "        xst.append(27*ix1 + ix2)\n",
        "        yst.append(ix3)\n",
        "\n",
        "xst = torch.tensor(xst)\n",
        "yst = torch.tensor(yst)\n",
        "num = len(xst)\n",
        "\n",
        "# Forward pass\n",
        "xenc = F.one_hot(xst, num_classes=729).float() # input to the network: one-hot encoding\n",
        "logits = xenc @ W # predict log-counts\n",
        "counts = logits.exp() # counts, equivalent to N matrix\n",
        "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "\n",
        "    # Loss: negative log-likelihood (NLL)\n",
        "    # We select the probability assigned to the actual next character\n",
        "losst = -probs[torch.arange(num), yst].log().mean() + 0.01*(W**2).mean() # Includes small regularizer\n",
        "print(f\"Test set loss: {losst.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2En_RLKSkNi",
        "outputId": "99098594-93c1-467d-f496-cd8818a040a0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set loss: 2.1974117755889893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once again, final loss with test and dev sets is similar to that of training set. This shows there's no particular overfitting that's observed."
      ],
      "metadata": {
        "id": "JZtSjkZ_Tztw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E03: L2 Regularization"
      ],
      "metadata": {
        "id": "xGJ2MiczT6Wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
        "\n",
        "# 1. Load the data\n",
        "words= open('names.txt', 'r').read().splitlines()\n",
        "num_words = len(words)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*num_words)\n",
        "n2 = int(0.9*num_words)\n",
        "train_set = words[:n1]\n",
        "dev_set = words[n1:n2]\n",
        "test_set = words[n2:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9dTP8W6kJ6Y",
        "outputId": "16dfe616-d536-4dde-c02d-6399ad75551d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-22 10:38:01--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt.1’\n",
            "\n",
            "\rnames.txt.1           0%[                    ]       0  --.-KB/s               \rnames.txt.1         100%[===================>] 222.80K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-12-22 10:38:01 (7.30 MB/s) - ‘names.txt.1’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Build the vocabulary and mappings\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "\n",
        "# 3. Create the dataset (xs: input character, ys: target next character)\n",
        "xs, ys = [], []\n",
        "for w in train_set:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "        ix1, ix2, ix3 = stoi[ch1], stoi[ch2], stoi[ch3]\n",
        "        xs.append(27*ix1 + ix2)\n",
        "        ys.append(ix3)\n",
        "\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)\n",
        "num = len(xs)\n",
        "\n",
        "# 4. Initialize the neural network (one-layer, 27 neurons)\n",
        "# We use a 729x27 weight matrix (mapping 729 inputs to 27 outputs)\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((729, 27), generator=g, requires_grad=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "tLcm1rboneoY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Gradient Descent (Training Loop)\n",
        "reg = 1\n",
        "for i in range(4):\n",
        "    for k in range(100):\n",
        "\n",
        "      # Forward pass\n",
        "      xenc = F.one_hot(xs, num_classes=729).float() # input to the network: one-hot encoding\n",
        "      logits = xenc @ W # predict log-counts\n",
        "      counts = logits.exp() # counts, equivalent to N matrix\n",
        "      probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "\n",
        "      # Loss: negative log-likelihood (NLL)\n",
        "      # We select the probability assigned to the actual next character\n",
        "      loss = -probs[torch.arange(num), ys].log().mean() + reg*(W**2).mean() # Includes small regularizer\n",
        "\n",
        "      # Backward pass\n",
        "      W.grad = None # set gradient to zero\n",
        "      loss.backward()\n",
        "\n",
        "      # Update weights\n",
        "      W.data += -50 * W.grad\n",
        "\n",
        "    print(f\"Reg = {reg}, Loss = {loss.item():.4f}\")\n",
        "    reg *= 0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xme-ju6uqNE6",
        "outputId": "cb957886-ef6c-4fed-dbed-9e831e1f1d59"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reg = 1, Loss = 2.8070\n",
            "Reg = 0.1, Loss = 2.2984\n",
            "Reg = 0.010000000000000002, Loss = 2.1995\n",
            "Reg = 0.0010000000000000002, Loss = 2.1647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Changing the value of regulariser and checking loss using dev set\n",
        "\n",
        "reg = 1\n",
        "xsd, ysd = [], []\n",
        "for w in dev_set:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "        ix1, ix2, ix3 = stoi[ch1], stoi[ch2], stoi[ch3]\n",
        "        xsd.append(27*ix1 + ix2)\n",
        "        ysd.append(ix3)\n",
        "\n",
        "xsd = torch.tensor(xsd)\n",
        "ysd = torch.tensor(ysd)\n",
        "num = len(xsd)\n",
        "\n",
        "# Forward pass\n",
        "xenc = F.one_hot(xsd, num_classes=729).float() # input to the network: one-hot encoding\n",
        "logits = xenc @ W # predict log-counts\n",
        "counts = logits.exp() # counts, equivalent to N matrix\n",
        "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "\n",
        "# Loss: negative log-likelihood (NLL)\n",
        "# We select the probability assigned to the actual next character\n",
        "for i in range(4):\n",
        "  lossd = -probs[torch.arange(num), ysd].log().mean() + reg*(W**2).mean() # Includes small regularizer\n",
        "  print(f\"Reg = {reg}, Dev set loss: {lossd.item():.4f}\")\n",
        "  reg *= 0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfB8j6EgnrWB",
        "outputId": "fd2f85b6-ab65-4b7f-ba70-b3ef50d75714"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reg = 1, Dev set loss: 2.8302\n",
            "Reg = 0.1, Dev set loss: 2.2442\n",
            "Reg = 0.010000000000000002, Dev set loss: 2.1856\n",
            "Reg = 0.0010000000000000002, Dev set loss: 2.1798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing Values:\n",
        "\n",
        "*   Reg = 1, Training set loss = 2.8070 , Dev set loss = 2.8302 , Difference = -0.0232\n",
        "*   Reg = 0.1, Training set loss = 2.2984 , Dev set loss = 2.2442 , Difference = +0.0542\n",
        "*   Reg = 0.01, Training set loss = 2.1995 , Dev set loss = 2.1856 , Difference = +0.0139\n",
        "*   Reg = 0.001, Training set loss = 2.1647 , Dev set loss = 2.1798 , Difference = -0.0151\n",
        "\n",
        "Since, the smallest difference is with reg = 0.01\n",
        "best_reg = 0.01\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WCfSNlF0uBBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loss with Test set\n",
        "best_reg = 0.01\n",
        "xst, yst = [], []\n",
        "for w in test_set:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "        ix1, ix2, ix3 = stoi[ch1], stoi[ch2], stoi[ch3]\n",
        "        xst.append(27*ix1 + ix2)\n",
        "        yst.append(ix3)\n",
        "\n",
        "xst = torch.tensor(xst)\n",
        "yst = torch.tensor(yst)\n",
        "num = len(xst)\n",
        "\n",
        "# Forward pass\n",
        "xenc = F.one_hot(xst, num_classes=729).float() # input to the network: one-hot encoding\n",
        "logits = xenc @ W # predict log-counts\n",
        "counts = logits.exp() # counts, equivalent to N matrix\n",
        "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "\n",
        "    # Loss: negative log-likelihood (NLL)\n",
        "    # We select the probability assigned to the actual next character\n",
        "losst = -probs[torch.arange(num), yst].log().mean() + best_reg*(W**2).mean() # Includes small regularizer\n",
        "print(f\"Test set loss: {losst.item():.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePTWLwsHvzVv",
        "outputId": "c826c785-4b12-4722-cc55-ad966922626f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set loss: 2.1974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E04: Replacement for One-Hot"
      ],
      "metadata": {
        "id": "CnMVnoxsw7ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# xenc = F.one_hot(xst, num_classes=729).float()\n",
        "# logits = xenc @ W\n",
        "\n",
        "logits = W[xs]\n",
        "# as xenc = 1 and was multiplied with the weight (=weight) (rest gave 0) only for the xth element which is stored in xs"
      ],
      "metadata": {
        "id": "Fu8Ie4B5x4R3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E05: F.cross_entropy"
      ],
      "metadata": {
        "id": "96lmEmLeyeZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xst, yst = [], []\n",
        "for w in test_set:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "        ix1, ix2, ix3 = stoi[ch1], stoi[ch2], stoi[ch3]\n",
        "        xst.append(27*ix1 + ix2)\n",
        "        yst.append(ix3)\n",
        "\n",
        "xst = torch.tensor(xst)\n",
        "yst = torch.tensor(yst)\n",
        "num = len(xst)\n",
        "\n",
        "# Forward pass\n",
        "logits = W[xst]\n",
        "loss = F.cross_entropy(logits, yst)\n",
        "\n",
        "# Loss: negative log-likelihood (NLL)\n",
        "# We select the probability assigned to the actual next character\n",
        "losst = -probs[torch.arange(num), yst].log().mean() + best_reg*(W**2).mean() # Includes small regularizer\n",
        "print(f\"Test set loss: {losst.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHoXX0ZDyduI",
        "outputId": "e5d82942-5960-4571-84bd-162d94e7dd6d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set loss: 2.1974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same loss as using spelled out probability and nll"
      ],
      "metadata": {
        "id": "ktJ119AK7tJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E06: Noisy Morse Denoiser\n",
        "The \"Noisy Signal\" Denoiser is a project that uses the Trigram model to recover original English names from corrupted Morse code. It works by balancing Morse match penalties with English letter probabilities to find the most likely intended message using a Beam Search algorithm.\n",
        "\n",
        "\n",
        "I wanted to try this idea out when I first thought of the application of trigrams to interpret Morse. Just predicting new names would not be as fun and could get too compliacted as the length of each character in Morse is different, so a change in the code from trigram would be required anyway.\n",
        "\n",
        "\n",
        "Instead, this project uses a 'noisy' input that wishes to convey a name but has some errors in the Morse code. Using the names.txt database of names, the project predicts the likely name the Morse code is trying to convey."
      ],
      "metadata": {
        "id": "WcPYa7EV79bS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. DOWNLOAD AND LOAD DATA\n",
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
        "\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "\n",
        "# 2. BUILD VOCABULARY AND MAPPINGS\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "\n",
        "# 3. BUILD THE 27x27x27 TRIGRAM COUNTS\n",
        "N = torch.zeros((27, 27, 27), dtype=torch.int32)\n",
        "\n",
        "for w in words:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "        ix1, ix2, ix3 = stoi[ch1], stoi[ch2], stoi[ch3]\n",
        "        N[ix1, ix2, ix3] += 1 # Fill counts matrix\n",
        "\n",
        "# Convert counts to Log-Probabilities using log_softmax\n",
        "# This handles normalization and numerical stability.\n",
        "P = F.log_softmax(N.float(), dim=2)\n",
        "\n",
        "# 4. MORSE DICTIONARY\n",
        "MORSE_DICT = {\n",
        "    'a': '.-', 'b': '-...', 'c': '-.-.', 'd': '-..', 'e': '.',\n",
        "    'f': '..-.', 'g': '--.', 'h': '....', 'i': '..', 'j': '.---',\n",
        "    'k': '-.-', 'l': '.-..', 'm': '--', 'n': '-.', 'o': '---',\n",
        "    'p': '.--.', 'q': '--.-', 'r': '.-.', 's': '...', 't': '-',\n",
        "    'u': '..-', 'v': '...-', 'w': '.--', 'x': '-..-', 'y': '-.--',\n",
        "    'z': '--..'\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtInB0AW_ulx",
        "outputId": "7971f1ca-f4ff-460d-8202-238cb73c10bf"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-22 17:38:28--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt.4’\n",
            "\n",
            "names.txt.4         100%[===================>] 222.80K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2025-12-22 17:38:28 (24.6 MB/s) - ‘names.txt.4’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_match_score(target_char, noisy_segment):\n",
        "    actual = MORSE_DICT.get(target_char, \"\")\n",
        "    # If lengths match, the penalty is small (based on dot/dash flips)\n",
        "    if len(actual) == len(noisy_segment):\n",
        "        diffs = sum(1 for a, b in zip(actual, noisy_segment) if a != b)\n",
        "        return -(diffs * 10)\n",
        "    # If lengths don't match, give a medium penalty\n",
        "    return -20"
      ],
      "metadata": {
        "id": "Tcx0ngeQ_4Pw"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. BEAM SEARCH DECODER\n",
        "def denoise_morse(noisy_morse_list, beam_width=5):\n",
        "    # Start with log-probability 0.0 and two start-of-word tokens '..'\n",
        "    beam = [(0.0, [0, 0])]\n",
        "\n",
        "    for noisy_seg in noisy_morse_list:\n",
        "        candidates = []\n",
        "\n",
        "        for current_score, path in beam:\n",
        "            prev1, prev2 = path[-2], path[-1] # Context from 3D matrix\n",
        "\n",
        "            # Try every possible letter A-Z\n",
        "            for char_idx in range(1, 27):\n",
        "                char_str = itos[char_idx]\n",
        "\n",
        "                # 1. Trigram Score: How likely is this letter in English names?\n",
        "                transition_score = P[prev1, prev2, char_idx].item()\n",
        "\n",
        "                # 2. Match Score: How well does it match our noisy signal?\n",
        "                match_score = get_match_score(char_str, noisy_seg)\n",
        "\n",
        "                # Combine (Add logs = Multiply probabilities)\n",
        "                total_score = current_score + transition_score + (match_score*5)\n",
        "                candidates.append((total_score, path + [char_idx]))\n",
        "\n",
        "        # Sort and prune candidates to keep only the best ones\n",
        "        candidates.sort(key=lambda x: x[0], reverse=True)\n",
        "        beam = candidates[:beam_width]\n",
        "\n",
        "    # Convert indices back to text\n",
        "    results = []\n",
        "    for score, path in beam:\n",
        "        clean_name = \"\".join([itos[i] for i in path[2:]])\n",
        "        results.append((clean_name, round(score, 2)))\n",
        "    return results"
      ],
      "metadata": {
        "id": "oVT2vMhc_7a3"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EXECUTION ---\n",
        "#Sending a noisy input - . -- ..\n",
        "noisy_input = [\"-\", \".\", \"--\", \"..\"]\n",
        "\n",
        "top_results = denoise_morse(noisy_input)\n",
        "\n",
        "print(\"Top Denoised Suggestions:\")\n",
        "for i, (name, score) in enumerate(top_results):\n",
        "    print(f\"{i+1}. {name.upper()} (Log-Prob Score: {score})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZHn9ZSDAAqE",
        "outputId": "ea0ce306-be59-447f-b3e3-277f71f9b838"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Denoised Suggestions:\n",
            "1. TALI (Log-Prob Score: -273.3)\n",
            "2. ELLA (Log-Prob Score: -303.3)\n",
            "3. TAMA (Log-Prob Score: -305.3)\n",
            "4. ELLI (Log-Prob Score: -319.3)\n",
            "5. TAMI (Log-Prob Score: -331.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given Noisy sample: - . -- ..  Translates to TEMI, which isn't a real name. These are the possible real names from the noisy input"
      ],
      "metadata": {
        "id": "uZULxAq2KmAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outputs:\n",
        "\n",
        "*   \"- .- .-.. ..\"\n",
        "*   \". .-.. .-.. .-\"\n",
        "*  \"- .- -- .- \"\n",
        "*   \". .-.. .-.. ..\"\n",
        "*   \"- .- -- ..\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Afrb32H2K3CZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sending a noisy input     .. . .- .- .--- --- IEAAJO\n",
        "#Most reasonable output    .. - .- .-.. .. .- ITALIA\n",
        "noisy_input = [\"..\", \".\", \".-\", \".-\", \".---\", \"---\"]\n",
        "\n",
        "top_results = denoise_morse(noisy_input)\n",
        "\n",
        "print(\"Top Denoised Suggestions:\")\n",
        "for i, (name, score) in enumerate(top_results):\n",
        "    print(f\"{i+1}. {name.upper()} (Log-Prob Score: {score})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edkGDLyLKlmi",
        "outputId": "1efd0e0a-1532-47f1-dcd4-f54af39a4483"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Denoised Suggestions:\n",
            "1. ITALEE (Log-Prob Score: -522.3)\n",
            "2. ITALIA (Log-Prob Score: -548.3)\n",
            "3. ITALIN (Log-Prob Score: -561.3)\n",
            "4. ITALEY (Log-Prob Score: -596.3)\n",
            "5. ITHARI (Log-Prob Score: -603.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "287Eg85KNiqn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
